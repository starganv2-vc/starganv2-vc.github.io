## VCTK Dataset
The following audios are converted using our model trained on 20 speakers from VCTK dataset. For a fair comparision to the baseline models, all audios are downsampled to 16k Hz. We demostrate four types of conversion schemes: many-to-many, any-to-many, cross-lingual and singing conversion.

**All utterancens are partially or completely unseen during training, and the results are uncurated (NOT cherry-picked)**. 

For more audio samples, please go to our survey used for MOS evaluation [here](https://survey.alchemer.com/s3/6266556/SoundQuality2).  You may have to randomly select some answers before proceeding to the next page.

---

### Many-to-Many Conversion

The converted samples from AUTO-VC are directly taken from the survey above. Because the survey uses different sources for different models in order to prevent the rater from finding out the ground truth, the audio clips shown below are converted from sources for AUTO-VC used in the survey. 

#### Male to Male

|         | Source | Target | AUTO-VC | StarGANv2-VC |
|:-------:|:------:|:------:|:-------:|:------------:|
| **Sample1** | <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/source.wav"></source> p243   </audio> |  <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/target.wav"></source> p245  |         |        <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/autovc.wav"></source>     | <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/starganv2.wav"></source>
| **Sample2** |        |        |         |              |

## VCTK Dataset
The following audios are converted using our model trained on 20 speakers from VCTK dataset. For a fair comparision to the baseline models, all audios are downsampled to 16k Hz. We demostrate four types of conversion schemes: many-to-many, any-to-many, cross-lingual and singing conversion.

**All utterancens are partially or completely unseen during training, and the results are uncurated (NOT cherry-picked)**. 

For more audio samples, please go to our survey used for MOS evaluation [here](https://survey.alchemer.com/s3/6266556/SoundQuality2).  You may have to randomly select some answers before proceeding to the next page.

---

### Many-to-Many Conversion

The converted samples from AUTO-VC are directly taken from the survey above. Because the survey uses different sources for different models in order to prevent the rater from finding out the ground truth, the audio clips shown below are converted from sources for AUTO-VC used in the survey. 

#### Male to Male

|              | Sample1 | Sample2 |
|:------------:|:-------:|:-------:|
|    **Source**    |    <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/source.wav"></source> p243   </audio>     |         |
|    **Target**    |     <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/target.wav"></source> p254   </audio>    |         |
|    **AUTO-VC**   |     <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/autovc.wav"></source> </audio>    |         |
| **StarGANv2-VC** |    <audio controls="controls">  <source type="audio/wav" src="https://raw.githubusercontent.com/starganv2-vc/starganv2-vc.github.io/main/wav/VCTK/Seen/M2M/p243xp254/starganv2.wav"></source> </audio>     |         |

